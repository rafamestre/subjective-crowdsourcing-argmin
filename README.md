# Benchmark Evaluation for Tasks with Highly Subjective Crowdsourced Annotations: Case study in Argument Mining of Political Debates

This is the accompanying dataset, codes and supplementary information of the paper "Benchmark Evaluation for Tasks with Highly Subjective Crowdsourced Annotations: Case study in Argument Mining of Political Debates", published at the 2nd NEATCLasS workshop 2023 at the 17th International AAAI Conference on Web and Social Media (ICWSM).




(Under construction)



# Instructions

The file "Instructions" contains the instructions used for the annotation tasks.

# Data

The folder "Data" contains the data obtained from the crowdsourced annotations, both the in-house and open crowd. There are some HTML files where the agreements and disagreements of the in-house annotations have been formated in a table, so that the reasons given by the annotators can be assessed.
